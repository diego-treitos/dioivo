#!/usr/bin/env python
# -*- coding:utf8 -*-
#
#  PyWench. A log driven web benchmarking tool.
#  Copyright (C) 2014  Diego Blanco
#
#  This program is free software: you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation, version 2 of the License.
#
#  This program is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with this program.  If not, see <http://www.gnu.org/licenses/>.
#

import urllib3
import httplib
import time
from multiprocessing import Process, Event, Queue, Manager
from random import Random
import sys
import os
if not os.getenv('DISPLAY'):
    import matplotlib as mpl
    mpl.use('Agg')
import matplotlib.pyplot as plt
import numpy as np
import pprint
from optparse import OptionParser

urllib3.disable_warnings( urllib3.exceptions.InsecureRequestWarning )

DEBUG=False
SOCKET_TIMEOUT=90

START_TIME=0
STOP_TIME=0


def get_connection_pool( host, http_version=11, auth_rule=None ):
    if http_version == 10:
        httplib.HTTPConnection._http_vsn = 10
        httplib.HTTPConnection._http_vsn_str = 'HTTP/1.0'
    elif http_version == 11:
        httplib.HTTPConnection._http_vsn = 11
        httplib.HTTPConnection._http_vsn_str = 'HTTP/1.1'
    else:
        raise PywenchException('Bad HTTP version: %s' % repr(http_version))

    if not auth_rule:
        cpool = urllib3.connectionpool.connection_from_url(host)
    else:
        try:
            auth = auth_rule.split('::')
            method=auth[0]
            url=auth[1]

            if url[:7].lower() != 'http://' or url[:8].lower() != 'https://':
                raise PywenchException('Bad authentication rule syntax: URL does not start by http:// or https://')

            params={}
            for p in auth[2:]:
                [k, v] = p.split('=')
                params[k]=v

            tpool = urllib3.connectionpool.connection_from_url(url)
            # Disable certificate verification for auth
            tpool.cert_reqs = 'CERT_NONE'
            tpool.ca_certs = None

            if method.lower() == 'get':
                r = tpool.request('GET', '%s?%s' % (url, '&'.join(auth[2:])))
                cookie = r.getheader('set-cookie')
            elif method.lower() == 'post':
                r = tpool.request('POST', '%s', fields=params)
                cookie = r.getheader('set-cookie')
            else:
                raise PywenchException('Bad authentication rule syntax: Only POST or GET methods allowed.')

            cpool = urllib3.connectionpool.connection_from_url(host, headers={'Cookie': cookie})

        except IndexError:
            raise PywenchException('Bad authentication rule syntax')

    # Disable certificate verification for all connections
    cpool.cert_reqs = 'CERT_NONE'
    cpool.ca_certs = None

    return cpool


def print_debug( msg, new_line=True):
    if new_line:
        print msg
    else:
        print msg,
    sys.stdout.flush()


class PywenchException(Exception):
    def __init__(self, msg):
        self.val = msg

    def __str__(self):
        return repr( self.val )


def request_worker( event, connection_variables, urls, data):
    """
    event : multiprocessing.Event
    connection_variables: dict
    urls: multiprocessing.Queue
    """
    event.wait()

    cp = get_connection_pool(
        connection_variables['host'],
        connection_variables['http_version'],
        connection_variables['auth_rule']
    )

    while not urls.empty():
        try:
            _url = urls.get()
            start = time.time()
            r = None
            r = cp.urlopen('GET', _url, retries=0, redirect=False, release_conn=False, preload_content=False, timeout=SOCKET_TIMEOUT)

            # get http code
            code = r.status

            # ttfb (time to first byte)
            r.read(1)
            ttfb = time.time() - start
            # ttlb (time to last byte)
            r.read()
            ttlb = time.time() - start

        except Exception as e:
            if not isinstance(e, (urllib3.exceptions.MaxRetryError)) and DEBUG:
                print_debug("\n[%d] Exception occurred: %s\n" % (os.getpid(), e.__repr__()))

            # In case of exception, set code=None so it does not affect timing stats
            code = None
            ttfb = 0
            ttlb = 0

        finally:
            # Release connection so the pool can continue doing requests
            if r is not None:
                r.release_conn()

            data.append(
                {
                    'time': start,
                    'url': _url,
                    'code': code,
                    'ttfb': ttfb,
                    'ttlb': ttlb
                }
            )


class UrlProvider(object):

    def __init__(self, urls_file_path, mode, requests_number, replace_parameter_list=[]):
        """mode = [random,squence]"""

        if mode.lower() not in ('random', 'sequence'):
            raise PywenchException('Invalid url choosing mode')

        self.requests_number = requests_number
        self.mode = mode.lower()
        self.replace_parameter_dict = {}
        self.__parse_replace_parameter( replace_parameter_list )

        try:
            self.queue = self.__filter_urls(urls_file_path)
        except IOError:
            raise PywenchException('Unable to open file: %s' % urls_file_path)

    def __parse_replace_parameter(self, rpl):
        for rp in rpl:
            npk, npv = rp.split('=')
            self.replace_parameter_dict[npk] = npv

    def __filter_urls(self, access_file):
        urls = []
        urls_to_use = Queue()
        r = Random()

        lines = open(access_file).readlines()
        for l in lines:
            if l.find('GET') != -1:
                urls.append(l.split()[6])

        url_index = None
        n_urls = urls.__len__()
        for i in range(0, self.requests_number):
            if self.mode == 'random':
                u = urls[ r.randint(0, n_urls-1) ]
            elif self.mode == 'sequence':
                # (Re)set the index
                if url_index is None or url_index == n_urls-1:
                    url_index = -1
                url_index+=1
                u = urls[ url_index ]

            url = u.strip()

            # If url is not a relative path, search for other url
            if url[:7].lower() == "http://" or url[:8].lower() == "https://":
                i+=1
                continue

            if self.replace_parameter_dict != {} and url.find('?') != -1:
                [rawurl, rawp] = url.split('?')
                params='?'
                for p in rawp.split('&'):
                    [k, v]=p.split('=')
                    if k in self.replace_parameter_dict:
                        params+="%s=%s&" % (k, self.replace_parameter_dict[k])
                    else:
                        params+="%s=%s&" % (k, v)
                # remove last &
                params=params[:-1]
                url = rawurl+params
            urls_to_use.put( url )

        return urls_to_use


class DataManager(object):

    def __init__(self, total_requests, concurrency, http_version):
        self.data = {}

        self.total_requests = total_requests
        self.concurrency = concurrency
        self.http_version = http_version

        self.plot_ttfb_data = []
        self.plot_ttlb_data = []
        self.plot_time_data = []
        self.stat = None
        self.output = ''

    def add_data(self, d):
        self.data[d['time']] = d
        self.plot_ttfb_data.append( d['ttfb'] )
        self.plot_ttlb_data.append( d['ttlb'] )
        self.plot_time_data.append( d['time'] - START_TIME )

    def save(self, filenames_prefix='output'):
        f = file(filenames_prefix+'.csv', 'w')
        dkeys = self.data.keys()
        dkeys.sort()
        f.write('time\t\turl\t\tttfb\t\tttlb\n')
        for k in dkeys:
            f.write('%f\t\t%s\t\t%f\t\t%f\n' % ( self.data[k]['time'], self.data[k]['url'], self.data[k]['ttfb'], self.data[k]['ttlb']))
        f.close()
        f = file(filenames_prefix+'.stat', 'w')
        pprint.pprint(self.stat, stream=f)
        f.close()

        f = file(filenames_prefix+'.txt', 'w')
        f.write(self.output)
        f.close()

        self.replot()
        plt.savefig('%s.png' % filenames_prefix, dpi=300)

    def compile_stats(self):
        self.stat={
                'ttfb': {
                    'min': None,
                    'avg': None,
                    'max': None
                },
                'ttlb': {
                    'min': None,
                    'avg': None,
                    'max': None
                },
                'url': {
                    'min_ttfb': None,
                    'max_ttfb': None,
                    'min_ttlb': None,
                    'max_ttlb': None
                },
                'code': {
                },
                'rps': None
        }

        # Stats for TTFB and TTLB
        count=0
        for d in self.data:
            # Init codes
            if self.data[d]['code'] in self.stat['code']:
                self.stat['code'][self.data[d]['code']]+=1
            else:
                self.stat['code'][self.data[d]['code']]=1

            # Avoid failed request to affect stats
            if self.data[d]['code'] is None:
                continue

            count+=1
            # Init
            if self.stat['ttfb']['min'] is None:
                self.stat['ttfb']['min'] = self.data[d]['ttfb']
                self.stat['ttfb']['avg'] = self.data[d]['ttfb']
                self.stat['ttfb']['max'] = self.data[d]['ttfb']
                self.stat['ttlb']['min'] = self.data[d]['ttlb']
                self.stat['ttlb']['avg'] = self.data[d]['ttlb']
                self.stat['ttlb']['max'] = self.data[d]['ttlb']
            else:
                # Min
                if self.data[d]['ttfb'] < self.stat['ttfb']['min']:
                    self.stat['ttfb']['min'] = self.data[d]['ttfb']
                if self.data[d]['ttlb'] < self.stat['ttlb']['min']:
                    self.stat['ttlb']['min'] = self.data[d]['ttlb']

                # Max
                if self.data[d]['ttfb'] > self.stat['ttfb']['max']:
                    self.stat['ttfb']['max'] = self.data[d]['ttfb']
                if self.data[d]['ttlb'] > self.stat['ttlb']['max']:
                    self.stat['ttlb']['max'] = self.data[d]['ttlb']

                # Avg
                self.stat['ttfb']['avg'] = self.stat['ttfb']['avg'] + (self.data[d]['ttfb']-self.stat['ttfb']['avg'])/float(count)
                self.stat['ttlb']['avg'] = self.stat['ttlb']['avg'] + (self.data[d]['ttlb']-self.stat['ttlb']['avg'])/float(count)

        req_count=count

        # self.stats for URL
        ud = {}
        for d in self.data:
            if self.data[d]['url'] not in ud:
                # Init ud for new url with ttfb, ttlb and the count of url occurrences (for mean calculation)
                ud[self.data[d]['url']]=[ self.data[d]['ttfb'], self.data[d]['ttlb'], 1]
            else:
                # Calculate mean values for ttfb and ttlb for this url
                ud[self.data[d]['url']][2] += 1
                count = ud[self.data[d]['url']][2]
                # 0: ttfb mean for this url
                ud[self.data[d]['url']][0] = ud[self.data[d]['url']][0] +  (self.data[d]['ttfb'] - ud[self.data[d]['url']][0])/count
                # 1: ttlb mean for this url
                ud[self.data[d]['url']][1] = ud[self.data[d]['url']][1] +  (self.data[d]['ttlb'] - ud[self.data[d]['url']][1])/count

        for k in ud:
            # Init
            if not self.stat['url']['min_ttfb']:
                self.stat['url']['min_ttfb'] = (ud[k][0], k)
                self.stat['url']['max_ttfb'] = (ud[k][0], k)
                self.stat['url']['min_ttlb'] = (ud[k][1], k)
                self.stat['url']['max_ttlb'] = (ud[k][1], k)
            else:
                if ud[k][0] < self.stat['url']['min_ttfb'][0]:
                    self.stat['url']['min_ttfb']=(ud[k][0], k)
                if ud[k][0] > self.stat['url']['max_ttfb'][0]:
                    self.stat['url']['max_ttfb']=(ud[k][0], k)
                if ud[k][1] < self.stat['url']['min_ttlb'][0]:
                    self.stat['url']['min_ttlb']=(ud[k][1], k)
                if ud[k][1] > self.stat['url']['max_ttlb'][0]:
                    self.stat['url']['max_ttlb']=(ud[k][1], k)

        # get requests per second
        self.stat['rps']=req_count/( STOP_TIME-START_TIME )

        return self.stat

    def print_stats(self):
        self.output+= 'Stats (seconds)\n\n'
        self.output+= '\t\tmin\t\tavg\t\tmax\n'
        self.output+= 'ttfb\t\t%.5f\t\t%.5f\t\t%.5f\n' % (self.stat['ttfb']['min'], self.stat['ttfb']['avg'], self.stat['ttfb']['max'])
        self.output+= 'ttlb\t\t%.5f\t\t%.5f\t\t%.5f\n' % (self.stat['ttlb']['min'], self.stat['ttlb']['avg'], self.stat['ttlb']['max'])
        self.output+= '\n\n'
        self.output+= 'Requests per second: %.3f\n' % self.stat['rps']
        self.output+= '\n\n'
        self.output+= 'URL min ttfb: (%.5f) %s\n' % self.stat['url']['min_ttfb']
        self.output+= 'URL max ttfb: (%.5f) %s\n' % self.stat['url']['max_ttfb']
        self.output+= 'URL min ttlb: (%.5f) %s\n' % self.stat['url']['min_ttlb']
        self.output+= 'URL max ttlb: (%.5f) %s\n' % self.stat['url']['max_ttlb']
        self.output+= 'NOTE: These stats are based on the average time (ttfb or ttlb) for each url.\n'
        self.output+= '\n\n'
        self.output+= 'Protocol stats:\n'
        code_list = self.stat['code'].keys()
        code_list.sort()
        for c in code_list:
            # Avoid None codes due to connection errors
            if c:
                self.output+= '\tHTTP %d: ' % c
            else:
                self.output+= '\tURL fail: '
            self.output+= '%5d requests (%5.2f%%)\n' % (self.stat['code'][c], 100*float(self.stat['code'][c])/self.total_requests)
        print self.output

    def replot(self):
        order = np.argsort( self.plot_time_data )
        xs = np.array( self.plot_time_data )[order]
        ttfb_ys = np.array( self.plot_ttfb_data )[order]
        ttlb_ys = np.array( self.plot_ttlb_data )[order]

        plt.title('Requests: %d   User concurrency: %d   HTTP %.1f' % (self.total_requests, self.concurrency, self.http_version/10.), loc='right')
        plt.xlabel('benchmark time (s)')
        plt.ylabel('request time (s)')
        plt.plot( xs, ttfb_ys, color="green", linewidth=1.0, linestyle="-", label="ttfb" )
        plt.plot( xs, ttlb_ys,  color="blue", linewidth=1.0, linestyle="-", label="ttlb" )
        plt.legend()


def pw_getopts():
    parser = OptionParser(usage="usage: %prog [options] -s SERVER -u URLS_FILE -c CONCURRENCY -n NUMBER_OF_REQUESTS")

    parser.add_option("-i", "--test-id",
            action="store",
            dest="test_id",
            type="string",
            default=False,
            help="Identificator for the test. This name will be a suffix for the output files.")

    parser.add_option("-s", "--server",
            action="store",
            dest="server",
            type="string",
            default=None,
            help="Server to benchmark. It must include the protocol and lack of trailing slash. For example: https://example.com")

    parser.add_option("-u", "--urls",
            action="store",
            dest="urls_file",
            type="string",
            default=False,
            help="File with url's to test. This file must be directly an access.log file from nginx or apache.'")

    parser.add_option("-c", "--concurrency",
            action="store",
            dest="concurrency",
            type="int",
            default=False,
            help="Number of concurrent requests")

    parser.add_option("-n", "--number-of-requests",
            action="store",
            dest="total_requests",
            type="int",
            default=False,
            help="Number of requests to send to the host.")

    parser.add_option("-m", "--mode",
            action="store",
            dest="mode",
            type="string",
            default='random',
            help="Mode can be 'random' or 'sequence'. It defines how the urls will be chosen from the url's file.")

    parser.add_option("-R", "--replace-parameter",
            action="append",
            dest="replace_parameter",
            type="string",
            default=[],
            help="Replace parameter on the URLs that have such parameter: p.e.: 'user=hackme' will set the parameter 'user' to 'hackme' on all url that have the 'user' parameter. Can be called several times to make multiple replacements.")

    parser.add_option("-A", "--auth",
            action="store",
            dest="auth_rule",
            type="string",
            default=None,
            help="Adds rule for form authentication with cookies. Syntax: 'METHOD::URL[::param1=value1[::param2=value2]...]'. For example: 'POST::http://example.com/login.py::user=root::pass=hackme'. NOTE: Use full url with protocol as in the example.")

    parser.add_option("-H", "--http-version",
            action="store",
            dest="http_version",
            type="int",
            default='11',
            help="Defines which protocol version to use. Use '11' for HTTP 1.1 and '10' for HTTP 1.0")

    parser.add_option("-l", "--live",
            action="store_true",
            dest="dynamic_plot",
            default=False,
            help="If you enable this flag, you'll be able to examine live the plot of the benchmark at the end of the benchmark")

    (opts, args) = parser.parse_args()

    if not opts.server:
        parser.error('Missing server to test (-s)')
    if not opts.urls_file:
        parser.error('Missing file with urls (-u)')
    if not opts.concurrency:
        parser.error('Concurrency not given (-c)')
    if not opts.total_requests:
        parser.error('Number of requests not given (-n)')

    return opts


def pw_main( opts ):
    print '\nSetting up...',
    sys.stdout.flush()
    # Set parameters
    urls_file=opts.urls_file
    mode=opts.mode
    total_requests=opts.total_requests
    concurrency=opts.concurrency
    host=opts.server
    http_version=opts.http_version
    dynamic_plot=opts.dynamic_plot
    replace_parameter=opts.replace_parameter
    replace_parameter
    auth_rule=opts.auth_rule
    test_id=opts.test_id

    # Create needed instances
    up = UrlProvider(urls_file, mode, total_requests, replace_parameter_list=replace_parameter)
    dm = DataManager(total_requests, concurrency, http_version)
    pm = Manager()
    raw_data = pm.list()
    event = Event()

    # Create processes for concurrency
    ps = []
    for p in xrange(concurrency):
        p = Process(
            target = request_worker,
            args = (
                event,
                {
                    'host': host,
                    'http_version': http_version,
                    'auth_rule': auth_rule
                },
                up.queue,
                raw_data
            )
        )
        ps.append( p )

    # Start processes
    # Set event so we ensure all threads start the benchmark
    # at the same time.
    for p in ps:
        p.start()
    global START_TIME
    START_TIME=time.time()
    event.set()

    print "\rRequests: %d\t\tConcurrency: %d\n" % (total_requests, ps.__len__())
    # bs: progress bar size
    bs=50

    # Main loop with progress bar and plotting
    time.sleep(1)
    last_qsize = up.queue.qsize()
    while not up.queue.empty():
        qsize = up.queue.qsize()
        per=(qsize*bs)/total_requests
        print ' [%s%s] %d rps     \r' % ('='*(bs-per), ' '*per, (last_qsize-qsize)*2),
        sys.stdout.flush()
        time.sleep(0.5)
        last_qsize = qsize
        sys.stdout.flush()
    print '%s              \n' % (' '*bs),
    sys.stdout.flush()
    global STOP_TIME
    STOP_TIME=time.time()

    # Wait for all proceses to stop
    print 'Stopping...',
    sys.stdout.flush()
    for p in ps:
        p.join()

    print "\rCompiling stats...",
    sys.stdout.flush()
    for d in raw_data:
        dm.add_data(d)
    dm.compile_stats()

    print "\r                  \r",
    sys.stdout.flush()
    dm.print_stats()

    # Saving information
    if test_id:
        file_prefix='%s_%s_r%d_c%d_http%d' %(test_id, host.split('://')[1], total_requests, concurrency, http_version)
    else:
        file_prefix='%s_r%d_c%d_http%d' %(host.split('://')[1], total_requests, concurrency, http_version)

    dm.save( file_prefix )

    if dynamic_plot:
        dm.replot()
        plt.show()


if __name__ == '__main__':
    try:
        opts = pw_getopts()
        pw_main( opts )
    except PywenchException as e:
        print 'Error:', e.__str__()
        sys.exit(1)
